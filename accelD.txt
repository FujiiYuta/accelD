accelD : ウェアラブルデバイスを用いた挨拶のジェスチャ認識と人間関係の親密度の測定と応用

藤井悠太1　渡邊恵

概要：人と対面する際, さまざまなジェスチャをする.我々は, これらのさまざまなジェスチャは対面する人同士の仲の良さや親密度などの関係性を示すと考えた. そこで, ウェアラブルデバイスによってジェスチャ認識を行い親密度を測ることで, その親密度に比例したコミュニケーションを促すシステム, accelDを提案する. accelDはウェアラブルデバイスとスマートフォンのみから構成するシステムである.

1.	はじめに 
　挨拶は, ほとんどの人が日常的に行う行為である. 挨拶には, ジェスチャを伴うことがある. たとえば, 友達であれば手を振ったりハイタッチをすることがある. 初対面の人であればお辞儀をしたり握手をしたりすることもある. 我々は, これらのさまざまなジェスチャは対面する人同士の仲の良さや親密度などの関係性を示すと考えた. 
　そこで本研究では, 腕に装着したウェアラブルデバイスを用いて, これらのジェスチャ認識をし, ジェスチャの種類や特徴, 強さから仲の良さや親密度といった人間関係の測定を試行する. 本研究ではこの親密度に基づき, 個人間の情報の公開ルールを段階的に変更するaccelDを提案する. accelDは, 機械学習における分類のアルゴリズム, k近傍法を用いることで, 多数のジェスチャを認識し, 判別する. 

2.	関連研究
　ジェスチャ認識研究において, ウェアラブルデバイスの加速度センサを用いて多くの研究が行われてきた[1][3][6]. 多くの研究では, 認識対象とするエンドユーザから事前にセンサデータを取得し, ラベル付けをしたサンプルデータを用いる手法が提案されている[2]. 本研究においても, 同様の想定をおく. また, これまでのジェスチャ認識研究の多くは, ユーザ自身がジェスチャの開始点と終了点を指定する必要があった[5]. しかし, 開始点と終了点を指定する動作自体を加速度データとして取得することは好ましくなく, 実際の利用環境を想定しても望ましくないため, 連続的なデータを抽出して分析する. 
 また, ジェスチャ認識の研究の多くでは, 機械学習の手法としてSVMを用いるものが多かったが, 本研究ではさまざまなジェスチャの種類を識別する必要があるため, k近傍法を用いる.


3. 	accelD

accelDは位置情報と複数のユーザのジェスチャ認識を同時刻に行うことで親密度を測る機能と, その親密度に基づいてコミュニケーションのきっかけを提示する機能に分かれている. 我々が行うジェスチャにはさまざまな種類があり(図1), 親密度の高いジェスチャほど付加する親密度を高くする. 
　ソフトウェアはiOSアプリケーションとして実装した. アプリケーション起動後の画面には他ユーザの一覧を表示(図2 左)し, ユーザ名をタップすることで特定ユーザとのジェスチャの履歴や他ユーザとの親密度を表示(図2 右)する.  親密度はユーザが行うジェスチャによって上がっていく. 
　位置情報はGPSによって取得する. iOSのフレームワーク, Core Locationを使用し, 位置情報を取得後に近距離通信フレームワーク, Multipeer Connectivityによりユーザ同士の通信を行う. 
　ジェスチャは, Apple Watchの加速度センサにより加速度を取得し, 機械学習モデルを作成することで認識する.
時系列データとしてCSV形式で保存している加速度データに任意のラベルをジェスチャごとにつけ, k近傍法によって機械学習モデルを作成する. iOSの機械学習フレームワーク, Core MLによってこの機械学習モデルを読み込む. 常に加速度データは取得し続け, ジェスチャも認識し続ける. 他ユーザとの距離が近く, ユーザ同士が互いにジェスチャをして繋がっているときに親密度を上げる処理を行う. 

4.	考察
　ジェスチャ認識を行い, それぞれのジェスチャについて認識率を示した(表1). 被験者1人が各ジェスチャ100回, 4つのジェスチャをApple Watchをつけて行った結果である. 
「握手」については96%認識することができた. 「手を振る」に関しては「手を挙げる」と誤認識することが10%近くあり, 78%の認識率となった. これは, 複数のユーザが同時刻, 近距離で何らかのジェスチャを開始してから2秒間の加速度を取得してジェスチャ認識をすることによるものである. 「手を挙げる」, 「ハイタッチをする」は互いに誤認識する確率が10~30%程度あり, 認識が困難な傾向にある. これは, 認識のための加速度取得時間, 取得する加速度の数によっても変わってくるだろうと考えている. 

5.	議論
　親密度が上がるのはジェスチャを行う場面だけでないことが多い. また, 特定のジェスチャによる親密度の上がり方は, 人によって差があると考えている. しかし, ジェスチャ以外の場面でユーザ同士が仲良くなった場合, ジェスチャがその親密度を反映するため, 結果としてaccelDが認識することが可能である. 
　本研究は, ジェスチャ認識によって, 親密度を数値化することを可能にした. 

5-1. 親密度
　挨拶のジェスチャだけでは親密度として不十分なことがある．たとえば，会う回数が少なくても，1回あたり長い時間話していれば，親密度は高まる可能性がある．したがって，会話時間やSNSでのやりとり，相互フォローなどの情報も踏まえることで，親密度にこれらをポイント化し加算することができるだろう. 
　accelDは, 立場が違うユーザ同士の関係性についても定義づけを行うことを想定している. 例えば, 上司と部下の関係では, 片方が手を振り, もう片方がお辞儀をするようなこともあるだろう. accelDでは, さまざまなジェスチャのうち, いずれかのジェスチャを同時刻にすることで, 親密度を付加する.

5.1.1	親密度を利用したアプリケーションの例
　対面で挨拶をする時にジェスチャから親密度を測ることで, ユーザ同士に親密度の付加値を示し(図3), ユーザが親密度に合わせたコミュニケーションをすることができる. 
　親密度の値を利用することで, たとえば親密度をベースとしたSNSを新たに作れる可能性がある. Facebookにはグループごとに投稿を切り替えることができるが, ユーザがわざわざ設定しなければならない. またそもそもSNSにおける友達は, これまでの日常生活の「友達」の概念とは異なる部分があり，申請し承諾すれば誰もが「友達」になってしまう. そこでaccelDによって, たとえば親密度を用いて, 親密度が10以上, 30以上, 50以上といった場合分けでグループに見える記事内容, 10以上のユーザは写真だけ, 30以上のユーザは音声覗いた動画も, 50以上のユーザには音声含む動画もといったような記事の見せ方ができる. こうすることで, 現在のFacebookのようにユーザが明示的にグループをつくることなく, 親密度に応じて段階的に向上する自然な友達関係を構築できる可能性がある. 
 また, Instagramでも親密度の高いユーザにしか見せないコンテンツがある. コンテンツだけでなく, 登録情報も柔軟に可変できるようにすると, accelDを用いることで親密度が10以上のユーザにはプロフィールの基本情報のみ, 30以上のユーザには年齢情報まで, 50以上のユーザには写真コンテンツ撮影場所の位置情報までを見せるようにすることができる. これも親密度による情報の段階的な開示である. 
5.1.2 情報の段階的制御
　ハイタッチをすることで, ユーザ同士の親密度が上がる. 親密度が上がったことでユーザ同士のコミュニケーションは親しみのあるものになる. accelDはコミュニケーションのきっかけとして, SNSの非公開アカウントのIDを互いに通知する. 最近, SNSで非公開アカウントだけでなく「親密度の高いユーザのみに公開するコンテンツ」の機能が流行している. accelDが親密度の高いユーザを制御することにより, 実際のコミュニケーションとSNS上のコミュニケーションを紐付ける仕組みが出来る. 親密度とaccelDが提示するコミュニケーションのきっかけは各ユーザの設定により, 柔軟に変えることができる. いずれは家の施錠権限や, より個人的な情報を交換する. 


参考文献

[1]	 Ling Bao and Stephen S. Intille.: Activity recognition from user-annotated acceleration data, Pervasive 2004, 2004, pp. 1-17.
[2]	 Martin Berchtold, Matthias Budde, Dawud Gordon, Hedda Schmidtke, and Michael Beigl.: ActiServ: Activity recognition service for mobile phone, International Symposium on Wearable Computers, 2010, pp. 1-8.
[3]	  Joseph Korpela, 前川卓也, Julien Eberle, Dipanjan Chakraborty, and Karl Aberer.: 身体に装着した加速度センサによる行動およびジェスチャの統合的認識手法の提案, 情報処理学会研究報告, vol.2014-HCI-160, No.1, 2014, p.1-8.
[4]	   Jiahui Wu, Gang Pan, Daqing Zhang, Guande Qi, and Shijian Li.: Gesture recognition with a 3-D accelerometer, Ubiquitous intelligence and computing, Springer, 2009, pp.25-38.
 Kent Lyons, Helene Brashear, Tracy Westeyn, Jung Soo Kim, and Thad Starner.: GART: The gesture and activity recognition toolkit, Human-Computer Interaction, HCI Intelligent Multimodal Interaction Environments, Springer, 2007, pp. 718-727.
[5]	 伊藤駿吾, 白石陽, 今野慎介, 手首装着型センサを用いた打鍵動作特徴による個人認証手法, 情報処理学会 DICOMO2016論文集, 2016, pp1165-1171.


